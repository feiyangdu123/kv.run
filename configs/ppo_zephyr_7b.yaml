apiVersion: mloc/v1
kind: TrainingTask
metadata:
  name: ppo-zephyr-7b-helpful
  owner: alice-smith
  project: rlhf-research
  annotations:
    description: "PPO training for Zephyr-7B to make it more helpful."
    
spec:
  taskType: "ppo"
  
  resources:
    replicas: 1
    hardware:
      cpu: "16"
      memory: "128Gi"
      gpu:
        type: "nvidia-a100-80gb"
        count: 8
  
  # Base model (SFT checkpoint)
  model:
    source:
      type: "huggingface"
      identifier: "HuggingFaceH4/zephyr-7b-beta"
      revision: "main"
    # Use LoRA for efficient training
    adapter:
      type: "lora"
      r: 32
      lora_alpha: 64
      lora_dropout: 0.1
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  
  # PPO dataset (prompts for generation)
  dataset:
    source:
      type: "huggingface"
      identifier: "Anthropic/hh-rlhf"
      split: "train"
    preprocessing:
      max_seq_length: 1024
  
  # PPO hyperparameters
  hyperparameters:
    output_dir: "/artifacts/ppo-zephyr-7b-helpful"
    num_train_epochs: 1
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 4
    learning_rate: 1e-5
    ppo_epochs: 4
    mini_batch_size: 32
    max_grad_norm: 1.0
    kl_penalty: "kl"
    target_kl: 0.1
    reward_model: "OpenAssistant/reward-model-deberta-v3-large-v2"
  
  output:
    destination:
      type: "huggingface"
      path: "my-org/zephyr-7b-helpful-ppo"
    artifacts:
      - "adapter_weights"
      - "full_model"
      - "training_logs"