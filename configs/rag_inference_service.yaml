apiVersion: mloc/v1
kind: TrainingTask
metadata:
  name: rag-inference-legal-docs
  owner: legal-team
  project: document-qa
  annotations:
    description: "RAG inference service for legal document Q&A."
    
spec:
  taskType: "rag_inference"
  
  resources:
    replicas: 1
    hardware:
      cpu: "8"
      memory: "32Gi"
      gpu:
        type: "nvidia-l40"
        count: 1
  
  # Language model for generation
  model:
    source:
      type: "huggingface" 
      identifier: "microsoft/DialoGPT-large"
      revision: "main"
  
  # Document collection for RAG
  dataset:
    source:
      type: "s3"
      identifier: "s3://legal-docs-bucket/processed_documents/"
    preprocessing:
      max_seq_length: 512
      chunk_size: 1000
      chunk_overlap: 200
  
  # RAG-specific parameters
  hyperparameters:
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    vector_db: "chromadb"
    vector_db_path: "/artifacts/vector_db"
    retrieval_k: 5
    max_tokens: 256
    temperature: 0.7
    port: 8080
  
  output:
    destination:
      type: "local"
      path: "/artifacts/rag_service"
    artifacts:
      - "vector_index"
      - "service_logs"